<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>QValue Reinforcement Learning with C# | NoamZeise</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="QValue Reinforcement Learning with C#" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Source Code on github I used Q-learning to make a program where an agent navigates a randomly generated board with obstacles to find and identify a goal. The video shows the behaviour of the untrained agent, then it is trained with 1,000,000 moves and is then shown navigating again. A table is made that includes all possible state transitions of the enviornment, and the expected reward from a certain state transition is tracked. The agent is rewarded for getting to the goal quickly, which updates the expected reward values in the state transition table. At the beginning the agent’s moves are essentially random, but as the expected reward values are updated the agent gets more and more accurate at finding the goal." />
<meta property="og:description" content="Source Code on github I used Q-learning to make a program where an agent navigates a randomly generated board with obstacles to find and identify a goal. The video shows the behaviour of the untrained agent, then it is trained with 1,000,000 moves and is then shown navigating again. A table is made that includes all possible state transitions of the enviornment, and the expected reward from a certain state transition is tracked. The agent is rewarded for getting to the goal quickly, which updates the expected reward values in the state transition table. At the beginning the agent’s moves are essentially random, but as the expected reward values are updated the agent gets more and more accurate at finding the goal." />
<link rel="canonical" href="http://localhost:4000/demo/2020/03/12/QLearning.html" />
<meta property="og:url" content="http://localhost:4000/demo/2020/03/12/QLearning.html" />
<meta property="og:site_name" content="NoamZeise" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-12T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="QValue Reinforcement Learning with C#" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-03-12T00:00:00+00:00","datePublished":"2020-03-12T00:00:00+00:00","description":"Source Code on github I used Q-learning to make a program where an agent navigates a randomly generated board with obstacles to find and identify a goal. The video shows the behaviour of the untrained agent, then it is trained with 1,000,000 moves and is then shown navigating again. A table is made that includes all possible state transitions of the enviornment, and the expected reward from a certain state transition is tracked. The agent is rewarded for getting to the goal quickly, which updates the expected reward values in the state transition table. At the beginning the agent’s moves are essentially random, but as the expected reward values are updated the agent gets more and more accurate at finding the goal.","headline":"QValue Reinforcement Learning with C#","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/demo/2020/03/12/QLearning.html"},"url":"http://localhost:4000/demo/2020/03/12/QLearning.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="NoamZeise" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">NoamZeise</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
	  <a class="page-link" href="https://github.com/NoamZeise">GitHub</a>
	  <a class="page-link" href="https://noamzeise.itch.io/">Itch.io</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">QValue Reinforcement Learning with C#</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-03-12T00:00:00+00:00" itemprop="datePublished">
        Mar 12, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/P1P63p1N3G4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<p><a href="https://github.com/NoamZeise/Q-LearningPathfinder/tree/master/RL%20project">Source Code on github</a></p>

<p>I used <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a> to make a program where an agent navigates a randomly generated board with obstacles to find and identify a goal. The video shows the behaviour of the untrained agent, then it is trained with 1,000,000 moves and is then shown navigating again.</p>

<p>A table is made that includes all possible state transitions of the enviornment, and the expected reward from a certain state transition is tracked. The agent is rewarded for getting to the goal quickly, which updates the expected reward values in the state transition table. At the beginning the agent’s moves are essentially random, but as the expected reward values are updated the agent gets more and more accurate at finding the goal.</p>

  </div><a class="u-url" href="/demo/2020/03/12/QLearning.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">

</footer>
</body>

</html>
